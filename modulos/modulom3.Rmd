---
title:  "Análisis de Regresión"
output:
  html_document:
    css: !expr here::here("styles/styles.css")
    toc: true
    toc_depth:  3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options("yaml.eval.expr" = TRUE)
```

# 3.3.Análisis de Regresión  

En el análisis de regresión se busca un modelo para describir estadísticamente la relación entre una __variable dependiente__ y otra (u otras) __independiente__.  El modelo es una ecuación matemática que "cuantifica" la relación entre las variables.  A la variable dependiente también se le llama __variable respuesta__ (y se grafica en el eje _y_) y a la variable independiente también se la conoce como __variable predictora__ o __explicativa__.  

A diferencia del análisis de correlación, en el análisis de regresión se asume una __relación causa-efecto__ en la que las variaciones en la variable predictora causan cambios en la variable respuesta.  

#### __Ejercicio__  
Escribir ejemplos de posibles relaciones causa-efecto entre dos variables, indicando cuál es la variable respuesta y cuál la predictora.

## 3.3.1.Fundamentos de la Regresión Lineal Simple
#### Objetivos  
__Formalizar el modelo y los supuestos de la regresión lineal simple__  

Una __regresión lineal simple__ modela el efecto de una variable predictora continua sobre una variable de respuesta continua. La __ecuación de regresión__ resultante se puede representar gráficamente como una __línea de regresión__, que representa los valores esperados de la variable de respuesta para todos los valores de la variable predictora.

En la regresión lineal simple, la relación funcional entre la variable dependiente (_y_) y la independiente (_x_) se representa:  
$$\mu_y = \alpha + \beta x$$

donde:  

> $\mu_y$: es la media poblacional de _y_ para cualquier valor de _x_,    
> $\alpha$: es el __intercepto__ y  
> $\beta$: es la __pendiente__.  

Esta ecuación describe una línea recta.  En forma estadística esta ecuación toma la forma:  
$$y_i = \alpha + \beta x + e_i$$  
donde el término $e_i$ indica la desviación aleatoria o __residual__ del valor de $y_i$ con respecto al valor esperado $\mu_y$.  

### Objetivos del Análisis de Regresión  

1. Estimar la ecuación de regresión mediante estimadores de $\alpha$ (_a_) y $\beta$ (_b_) a partir de una muestra y generar intervalos de confianza para los mismos.  
2. Estimar que tanto está la variable dependiente controlada por la variable independiente, en otras palabras que tanto la variación en _y_ se explica por la variación en _x_.  
3. Usar la ecuación de regresión para predecir valores de _y_ a partir de valores de _x_. 

### Supuestos del Análisis de Regresión  
Como en todo análisis estadístico paramétrico, la regresión lineal simple posee algunos supuestos:  

1. Las observaciones son independientes, lo cual implica que cada sujeto en una muestra solo se mide una vez: no puede haber pseudoreplicación.    
2. Se asume (aunque la visualización no lo muestre) que la relación entre las dos variables es lineal.  
3. Los residuales ($e_i$) alrededor de la línea de regresión tienen una distribución normal estándar ($\mu = 0$).  
4. La varianza de los residuales es igual para todos los valores de _x_ de los datos.  Esto es equivalente a la homogeneidad de varianza en el ANOVA.  

\

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.2.Parámetros de la Regresión
#### Objetivos
__Calcular y probar hipótesis sobre los estimadores de los parámetros de la regresión lineal simple__  

### Método de los Cuadrados Mínimos (_Least Squares_)  

El primer objetivo del análisis de regresión, encontrar los valores de _a_ y _b_ de la ecuación de la recta, requiere un procedimiento matemático para encontrar la "mejor" línea recta que pase por los puntos $(x_i,y_i)$ de las variables respuesta y predictora.  Comunmente se utiliza el método de los __cuadrados mínimos ('least squares')__, el cual consiste en encontrar el mínimo valor de esta expresión:  
$$\sum_{i=1}^n (y_i - \hat{y_i})^2$$
es decir el mínimo valor de la suma de las diferencias verticales de la _y_ de cada punto y la _y_ sobre una línea de regresión, elevada al cuadrado (Figura 1).  

![](./imagenes/14-2 Variation in Dependent Variable.jpg)  
\
__Figura 1.__ Ritmo cardíaco (BPM) en función de la temperatura corporal (ºC) en pitones. Línea con la menor distancia vertical a los puntos de la muestra ($y_i - \hat{y_i}$).  

### Cálculos del Coeficiente de Regresión (_b_) y el Intercepto en el eje Y (_a_)
Para el cálculo de los estimadores de $\alpha$ y $\beta$, según el método de los cuadrados mínimos, se necesitan las siguientes cantidades:  

__Suma de Cuadrados de la Variable Predictora, $\sum x^2$__  
(todas las sumatorias son de _i_ = 1 hasta _n_, número de puntos)  
$$\sum(X_i - \bar X)^2 = \sum X_i^{2} - \frac{(\sum X_i)^2}{n}  $$

__Suma de Productos Cruzados, $\sum xy$__
$$\sum(X_i - \bar X)(Y_i - \bar Y) = \sum X_iY_i - \frac{(\sum X_i)(\sum Y_i)}{n}$$
El __coeficiente de regresión__ (parámetro $\beta$) es la pendiente de la __línea de regresión__ y su estimador (_b_) es el siguiente:  
$$b = \frac{\sum xy}{\sum x^2}$$
Para calcular el estimador de $\alpha$, _a_, utilizamos el estimador _b_ y las medias de _x_ y _y_ (que por definición están en la recta de regresión):
$$a = \bar Y - b\bar X$$



\  

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.3.Pruebas de Hipótesis para la Regresión

 

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.4.Predicción de y a partir de x

 

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.5.Otras Técnicas de Regresión

  

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)
