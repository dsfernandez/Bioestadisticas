---
title:  "Análisis de Regresión"
output:
  html_document:
    css: !expr here::here("styles/styles.css")
    toc: true
    toc_depth:  3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options("yaml.eval.expr" = TRUE)
```

# 3.3.Análisis de Regresión  

En el análisis de regresión se busca un modelo para describir estadísticamente la relación entre una __variable dependiente__ y otra (u otras) __independiente__.  El modelo es una ecuación matemática que "cuantifica" la relación entre las variables.  A la variable dependiente también se le llama __variable respuesta__ (y se grafica en el eje _Y_) y a la variable independiente también se la conoce como __variable predictora__ o __explicativa__.  

A diferencia del análisis de correlación, en el análisis de regresión se asume una __relación causa-efecto__ en la que las variaciones en la variable predictora causan cambios en la variable respuesta.  

#### __Ejercicio__  
Escribir ejemplos de posibles relaciones causa-efecto entre dos variables, indicando cuál es la variable respuesta y cuál la predictora.

## 3.3.1.Fundamentos de la Regresión Lineal Simple
#### Objetivos  
__Formalizar el modelo y los supuestos de la regresión lineal simple__  

Una __regresión lineal simple__ modela el efecto de una variable predictora continua sobre una variable de respuesta continua. La __ecuación de regresión__ resultante se puede representar gráficamente como una __línea de regresión__, que representa los valores esperados de la variable de respuesta para todos los valores de la variable predictora.

En la regresión lineal simple, la relación funcional entre la variable dependiente (_y_) y la independiente (_x_) se representa:  
$$\mu_y = \alpha + \beta x$$

donde:  

> $\mu_y$: es la media poblacional de _y_ para cualquier valor de _x_,    
> $\alpha$: es el __intercepto__ y  
> $\beta$: es la __pendiente__.  

Esta ecuación describe una línea recta.  En forma estadística esta ecuación toma la forma:  
$$y_i = \alpha + \beta x + e_i$$  
donde el término $e_i$ indica la desviación aleatoria o __residual__ del valor de $y_i$ con respecto al valor esperado $\mu_y$.  

### Objetivos del Análisis de Regresión  

1. Estimar la ecuación de regresión mediante estimadores de $\alpha$ (_a_) y $\beta$ (_b_) a partir de una muestra y generar intervalos de confianza para los mismos.  
2. Estimar que tanto está la variable dependiente controlada por la variable independiente, en otras palabras que tanto la variación en _y_ se explica por la variación en _x_.  
3. Usar la ecuación de regresión para predecir valores de _y_ a partir de valores de _x_. 

### Supuestos del Análisis de Regresión  
Como en todo análisis estadístico paramétrico, la regresión lineal simple posee algunos supuestos:  

1. Las observaciones son independientes, lo cual implica que cada sujeto en una muestra solo se mide una vez: no puede haber pseudoreplicación.    
2. Se asume (aunque la visualización no lo muestre) que la relación entre las dos variables es lineal.  
3. Los residuales ($e_i$) alrededor de la línea de regresión tienen una distribución normal estándar ($\mu = 0$).  
4. La varianza de los residuales es igual para todos los valores de _x_ de los datos.  Esto es equivalente a la homogeneidad de varianza en el ANOVA.  

\

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.2.Parámetros de la Regresión
#### Objetivos
__Calcular y probar hipótesis sobre los estimadores de los parámetros de la regresión lineal simple__  

### Método de los Cuadrados Mínimos (_Least Squares_)  

El primer objetivo del análisis de regresión, encontrar los valores de _a_ y _b_ de la ecuación de la recta, requiere un procedimiento matemático para encontrar la "mejor" línea recta que pase por los puntos $(x_i,y_i)$ de las variables respuesta y predictora.  Comunmente se utiliza el método de los __cuadrados mínimos ('least squares')__, el cual consiste en encontrar el mínimo valor de esta expresión:  
$$\sum_{i=1}^n (y_i - \hat{y_i})^2$$
es decir el mínimo valor de la suma de las diferencias verticales de la _y_ de cada punto y la _y_ sobre una línea de regresión, elevada al cuadrado (Figura 1).  

![](./imagenes/14-2 Variation in Dependent Variable.jpg)  
\
__Figura 1.__ Ritmo cardíaco (BPM) en función de la temperatura corporal (ºC) en pitones. Línea con la menor distancia vertical a los puntos de la muestra ($y_i - \hat{y_i}$).  

### Cálculos del Coeficiente de Regresión (_b_) y el Intercepto en el eje Y (_a_)
Para el cálculo de los estimadores de $\alpha$ y $\beta$, según el método de los cuadrados mínimos, se necesitan las siguientes cantidades:  

__Suma de Cuadrados de la Variable Predictora, $\sum x^2$__  
(todas las sumatorias son de _i_ = 1 hasta _n_, número de puntos)  
$$\sum(X_i - \bar X)^2 = \sum X_i^{2} - \frac{(\sum X_i)^2}{n}  $$

__Suma de Productos Cruzados, $\sum xy$__
$$\sum(X_i - \bar X)(Y_i - \bar Y) = \sum X_iY_i - \frac{(\sum X_i)(\sum Y_i)}{n}$$
El __coeficiente de regresión__ (parámetro $\beta$) es la pendiente de la __línea de regresión__ y su estimador (_b_) es el siguiente:  
$$b = \frac{\sum xy}{\sum x^2}$$
Para calcular el estimador de $\alpha$, _a_, utilizamos el estimador _b_ y las medias de _x_ y _y_ (que por deducción matemática, están en la recta de regresión):
$$a = \bar Y - b\bar X$$

\  

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)


#### __Ejemplo 1__  
Vamos a usar los datos de la longitud (cm) del ala derecha de gorriones de diferentes edades (días).  En este caso, podemos asumir que la variable dependiente es la longitud del ala y la independiente, la edad de las aves.  
```{r gorriones}
edad <- c(3.0,4.0,5.0,6.0,8.0,9.0,10.0,11.0,12.0,14.0,15.0,16.0,17.0)
l.ala <- c(1.4,1.5,2.2,2.4,3.1,3.2,3.2,3.9,4.1,4.7,4.5,5.2,5.0)
sparrow <- data.frame(edad,l.ala)
#tabla de datos
knitr::kable(sparrow, col.names = c("Edad (días)", "Longitud ala (cm)"), caption = 'Tabla 1.  Longitud del ala derecha de gorriones de diferente edad') 
#gráfica x-y
plot(edad,l.ala,xlab = "Edad, días", ylab = "Longitud, cm")
```
\
__Figura 2.__ Gráfica de los datos de la Tabla 1, longitud del ala de gorriones (variable dependiente) y edad (variable independiente).  

Observando la Figura 1 podemos sospechar que hay una posible relación entre la longitud del ala y la edad.  A continuación calcularemos los estadísticos que necesitamos para obtener los estimadores de los parámetros de la ecuación de regresión lineal para las variables de esta muestra.  

### Cálculo Manual

```{r calcab}
#tamaño muestra
n <- length(l.ala)
#medias
X <- mean(sparrow$edad)
Y <- mean(sparrow$l.ala)
#sumatorias
sumX <- sum(sparrow$edad)
sumY <- sum(sparrow$l.ala)
sumX2 <- sum(sparrow$edad^2)
sumXY <- sum(sparrow$edad*sparrow$l.ala)
#suma de cuadrados
SSx2 <- sumX2 - (sumX^2)/n
Sxy <- sumXY - (sumX*sumY)/n
#cálculo de b
est.b <- Sxy/SSx2
# cálculo de a  
est.a = Y - est.b*X
```

__Resultados parciales__  
$\sum x_i$ = `r sumX`  
$\sum y_i$ = `r sumY`  
$\sum {x_i}^2$ = `r sumX2`  
$\sum x_i y_i$ = `r sumXY`    
$\sum x^2$ (suma de cuadrados) = `r SSx2`  
$\sum xy$ (suma de productos cruzados) = `r Sxy`    

__Coeficiente de Regresión (_b_)__    
$b$ = `r est.b`  

__Intercepto en Y (_a_)__    
$a$ = `r est.a`  

__Ecuación de la recta de regresión__  
```{r echo=FALSE, message=FALSE, warning=FALSE}
sprintf("Y = %.3f + %.3fX", est.a, est.b)
```

### Cálculo de Coeficientes con R
Para obtener los coeficientes de la recta de regresión, usamos la función __lm__ (_linear models_) en R.
```{r lmreg}
regr <- lm(l.ala ~ edad)
coefficients(regr)
```
### Gráfica de la línea de regresión lineal

```{r}
plot(edad,l.ala, xlab = 'Edad, días', ylab = 'Longitud del ala, cm', asp = 1)
abline(regr, col = 'blue')
```
\
__Figura 3.__ Línea de regresión para la relación entre la longitud del ala (cm) y la edad (días) de gorriones.

\  

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.3.Pruebas de Hipótesis y Evaluación de la Regresión

 

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.4.Predicción de y a partir de x

 

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)

## 3.3.5.Otras Técnicas de Regresión

  

[  __Home__](https://dsfernandez.github.io/bioestadisticas/index.html)
